{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da17030-a518-4bfa-ad29-c4c6bbe92a02",
   "metadata": {},
   "source": [
    "# Featout implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caada4ab-3876-45ed-9252-f058db927cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from scipy.signal import convolve2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b6da6f-e59c-4d36-b6ca-76a58dfcca40",
   "metadata": {},
   "source": [
    "#### Gradient-based method to infer the attention of a model on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb79ae19-4581-4b38-9b6b-05fa9e47e020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import Saliency\n",
    "\n",
    "def simple_gradient_saliency(net, input_img, label):\n",
    "    \"\"\"\n",
    "    Simplest interpretable method\n",
    "    Takes an image and computes the gradients\n",
    "    \"\"\"\n",
    "    initial_input = input_img.clone()\n",
    "    # activate gradients\n",
    "    initial_input.requires_grad = True\n",
    "    net.eval()\n",
    "    # saliency method --> can use other method here\n",
    "    saliency = Saliency(net)\n",
    "    grads = saliency.attribute(initial_input, target=label)\n",
    "    return grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc587086-f271-4798-9bcb-f2cb6890af63",
   "metadata": {},
   "source": [
    "### Zero-out feature\n",
    "\n",
    "This is the code to find the maximum coordinates of the activation and \"delete\" a patch around these coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10562dc2-7f91-4608-ba60-f4f731de8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_activation(gradients, filter_size=3):\n",
    "    \"\"\"\n",
    "    Get the coordinates & value where the activation is maximal\n",
    "    Includes smoothing with an all-ones filter of size filter_size\n",
    "    \"\"\"\n",
    "    grads_mean = np.mean(gradients, axis=0)\n",
    "    # smooth the results to avoid using outlier activation\n",
    "    filtered = convolve2d(\n",
    "        grads_mean,\n",
    "        np.ones((filter_size, filter_size)),\n",
    "        mode=\"same\",\n",
    "    )\n",
    "\n",
    "    flat_filtered = filtered.flatten()\n",
    "\n",
    "    # get max of smoothed array\n",
    "    max_act = np.argmax(flat_filtered)\n",
    "    max_val = flat_filtered[max_act]\n",
    "    # get corresponding x and y coordinates\n",
    "    max_x = max_act // grads_mean.shape[1]\n",
    "    max_y = max_act % grads_mean.shape[1]\n",
    "    \n",
    "    return max_x, max_y, max_val\n",
    "    \n",
    "def zero_out(img, patch_mask):\n",
    "    \"\"\"\n",
    "    Zero out a quadratic patch around max_coordinates with a Gaussian filter\n",
    "    \"\"\"\n",
    "    # max_x, max_y = max_coordinates\n",
    "    # modified_input = img.clone()\n",
    "    img[0][patch_mask] = 0\n",
    "    # modified_input[:, :, max_x - patch_radius:max_x + patch_radius + 1,\n",
    "    #                max_y - patch_radius:max_y + patch_radius + 1] = 0\n",
    "    # return modified_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb74c193-d0e9-4ecc-a68b-9a87057c1b61",
   "metadata": {},
   "source": [
    "### FEATOUT dataset\n",
    "\n",
    "This is the core of the pipeline, a torch Dataset class that applies featout in the `__getitem__` method. In other words, everytime that a new batch is sampled, the  `__getitem__` method from this dataset is called. If featout is activated (class attribute `featout=True`), then the method applies an attention mechanism to find the features with highest activation, and blurs the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e5c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_patch(x, y, val, img_shape, radius=4):\n",
    "    mask = np.full(img_shape, False)\n",
    "    mask[x-radius:x+radius+1, y-radius:y+radius+1] = True\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6b4b1-2aa2-414e-896d-98009298cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Featout(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Inspired from https://discuss.pytorch.org/t/changing-transformation-applied-to-data-during-training/15671/3\n",
    "    Example usage:\n",
    "    dataset = Featout(normal arguments of super dataset)\n",
    "    loader = DataLoader(dataset, batch_size=2, num_workers=2, shuffle=True)\n",
    "    loader.dataset.start_featout(net)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataset, plotting_path, *args, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: torch Dataset object (must impelemnt getitem and len)\n",
    "        \"\"\"\n",
    "        # actual dataset\n",
    "        self.dataset = dataset\n",
    "        # initial stage: no blurring\n",
    "        self.featout = False\n",
    "        # set path where to save plots (set to None if no plotting desired)\n",
    "        self.plotting = plotting_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Main workflow: Get image, if label correct, then featout (blur/zero)\n",
    "        and return the modified image\n",
    "        \"\"\"\n",
    "        # call method from base dataset\n",
    "        image, label = self.dataset.__getitem__(index)\n",
    "\n",
    "        if self.featout:\n",
    "            in_img = torch.unsqueeze(image, 0)\n",
    "\n",
    "            # run a prediction with the given model --> TODO: this can be done\n",
    "            # more efficiently by passing the predicted labels from the\n",
    "            # preceding epoch to this class\n",
    "            _, predicted_label = torch.max(\n",
    "                self.featout_model(in_img).data, 1\n",
    "            )\n",
    "            # only do featout if it was predicted correctly\n",
    "            if predicted_label == label:\n",
    "                # get model attention via gradient based method\n",
    "                gradients = self.algorithm(\n",
    "                    self.featout_model, in_img, label\n",
    "                )[0].numpy()\n",
    "\n",
    "                blurred_image = in_img.clone()\n",
    "\n",
    "                for i in range(self.n_patches):\n",
    "                    max_x, max_y, val = get_max_activation(gradients)\n",
    "                    patch = self.patch_select(max_x, max_y, val, (in_img.shape[1], in_img.shape[2]))\n",
    "                    gradients[patch] = 0\n",
    "                    self.blur_method(blurred_image, patch)\n",
    "\n",
    "            \n",
    "                # blur patch at activation (feat-out)\n",
    "                # blurred_image = self.blur_method(\n",
    "                #     in_img, patch\n",
    "                # )\n",
    "\n",
    "                # save images before and after if plotting is desired\n",
    "                if PLOTTING:\n",
    "                    new_grads = self.algorithm(\n",
    "                        self.featout_model,\n",
    "                        blurred_image,\n",
    "                        label,\n",
    "                    )[0].numpy()\n",
    "                    plot_together(\n",
    "                        image,\n",
    "                        gradients,\n",
    "                        blurred_image[0],\n",
    "                        new_grads\n",
    "                    )\n",
    "\n",
    "                image = blurred_image[0]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def start_featout(\n",
    "        self,\n",
    "        model,\n",
    "        n_patches=3,\n",
    "        patch_select=square_patch,\n",
    "        blur_method=zero_out,\n",
    "        algorithm=simple_gradient_saliency,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        We can set here whether we want to blur or zero and what gradient alg\n",
    "        \"\"\"\n",
    "        print(\"\\n STARTING FEATOUT \\n \")\n",
    "        self.featout = True\n",
    "        self.n_patches = n_patches\n",
    "        self.algorithm = algorithm\n",
    "        self.featout_model = model\n",
    "        self.patch_select = patch_select\n",
    "        self.blur_method = blur_method\n",
    "\n",
    "    def stop_featout(self):\n",
    "        self.featout = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0b1ab3-c5ca-47dc-8d2d-0791f504efdc",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "Here is some code that we use for visualizing what featout does during the epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfecd944-d839-433c-8ff7-4dc198bfaf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_cifar(img):\n",
    "    \"\"\"for cifar, we need to transform the images\"\"\"\n",
    "    return np.transpose(\n",
    "        (img.cpu().detach().numpy() / 2) + 0.5, (1, 2, 0)\n",
    "    )\n",
    "\n",
    "def img_to_npy(img):\n",
    "    return img.squeeze().cpu().detach().numpy()\n",
    "\n",
    "\n",
    "def get_overlayed_img(image, gradients):\n",
    "    \"\"\"\n",
    "    Normalize gradients and overlay image with them (red channel)\n",
    "    \"\"\"\n",
    "    normed_gradients = np.mean(gradients, axis=0)\n",
    "    normed_gradients = (\n",
    "        normed_gradients - np.min(normed_gradients)\n",
    "    ) / (\n",
    "        np.max(normed_gradients) - np.min(normed_gradients)\n",
    "    )\n",
    "    # Take image in greyscale\n",
    "    transformed = transform_cifar(image)\n",
    "    overlayed = np.tile(\n",
    "        np.expand_dims(\n",
    "            np.mean(transformed, axis=2).copy(), 2\n",
    "        ),\n",
    "        (1, 1, 3),\n",
    "    )\n",
    "    # colour the gradients red\n",
    "    overlayed[:, :, 0] = normed_gradients\n",
    "    return overlayed\n",
    "\n",
    "\n",
    "def plot_together(\n",
    "    image,\n",
    "    gradients,\n",
    "    blurred_image,\n",
    "    new_grads,\n",
    "    save_path=\"outputs/test.png\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot four images: the original one, then overlayed by gradients, then the\n",
    "    blurred one, then this one overlayed by the new gradients\n",
    "    \"\"\"\n",
    "    # get the points of max activation\n",
    "    max_x, max_y, _ = get_max_activation(gradients)\n",
    "    new_max_x, new_max_y, _ = get_max_activation(new_grads)\n",
    "    # Make figure\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(img_to_npy(image))\n",
    "    plt.title(\"Original input image\")\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(get_overlayed_img(image, gradients))\n",
    "    plt.title(\n",
    "        f\"Model attention BEFORE blurring (max at x={max_x}, y={max_y})\"\n",
    "    )\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(img_to_npy(blurred_image))\n",
    "    plt.title(\"Modified input image (blurred)\")\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.imshow(get_overlayed_img(blurred_image, new_grads))\n",
    "    plt.title(\n",
    "        f\"Model attention AFTER blurring (max at x={max_x}, y={max_y})\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e18f13-42f2-4931-b9ea-e9630f69f591",
   "metadata": {},
   "source": [
    "## Run pipeline\n",
    "\n",
    "We are now ready to put everything together. This code trains on the MNIST dataset and starts and stops featout inbetween."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33199717-b0b7-42e7-91e1-35a9c079d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = torchvision.datasets.MNIST  # CIFAR10\n",
    "# method how to remove features - here by default blurring\n",
    "BLUR_METHOD = zero_out\n",
    "# algorithm to derive the model's attention\n",
    "ATTENTION_ALGORITHM = simple_gradient_saliency\n",
    "# set this path to some folder, e.g., \"outputs\", if you want to plot the results\n",
    "PLOTTING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85690819-4eb2-4151-b849-b42ddcff65b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# augmentation\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        # for cifar: (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load the original dataset (it's downloaded automatically if not found)\n",
    "original_trainset = DATASET(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "# wrap the dataset with featout\n",
    "trainset = Featout(original_trainset, None)\n",
    "\n",
    "# the trainloader handles batching of the training set\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=4, shuffle=True, num_workers=0\n",
    ")\n",
    "# for the test data, we don't need any transformations, so we take the original\n",
    "# dataset and put it into the dataloader (without shuffling)\n",
    "testset = DATASET(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=4, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "# define a simple CNN\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 5, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(5, 10, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(160, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 160)\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "# define model and optimizer (standard mnist model from torch is used)\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), lr=0.001, momentum=0.9\n",
    ")\n",
    "\n",
    "for epoch in range(2):\n",
    "    tic = time.time()\n",
    "    running_loss = 0.0\n",
    "    blurred_set = []\n",
    "\n",
    "    # # START FEATOUT\n",
    "    # # TODO: decide when to start featout, or only every second epoch?\n",
    "    # if epoch > 0:\n",
    "    trainloader.dataset.start_featout(\n",
    "            net,\n",
    "            blur_method=BLUR_METHOD,\n",
    "            algorithm=ATTENTION_ALGORITHM,\n",
    "    )\n",
    "\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (\n",
    "            i % 2000 == 1999\n",
    "        ):  # print every 2000 mini-batches\n",
    "            print(\n",
    "                \"Epoch %d, samples %5d] loss: %.3f\"\n",
    "                % (epoch + 1, i + 1, running_loss / 2000)\n",
    "            )\n",
    "            running_loss = 0.0\n",
    "\n",
    "    print(f\"time for epoch: {time.time()-tic}\")\n",
    "\n",
    "    # Evaluate test performance\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(\n",
    "        \"Accuracy of the network on the test images: %d %%\"\n",
    "        % (100 * correct / total)\n",
    "    )\n",
    "\n",
    "    # stop featout after every epoch\n",
    "    trainloader.dataset.stop_featout()\n",
    "\n",
    "# Save model\n",
    "print(\"Finished Training\")\n",
    "torch.save(\n",
    "    net.state_dict(), \"trained_models/cifar_torchvision.pt\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6cbe0e-f7f3-4ffd-a168-1230c6b48a95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
